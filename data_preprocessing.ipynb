{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c6d0426",
   "metadata": {},
   "source": [
    "#### 1.1 Reorganize all csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3505970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.\\\\Data\\\\城市_20250101-20250329']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 142.03it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from glob import glob  \n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "data_root = r\".\\Data\"\n",
    "directories = glob(os.path.join(data_root, r'城市*'))\n",
    "print(directories)\n",
    "for directory in tqdm(directories):\n",
    "    if os.path.exists(os.path.join(directory, Path(directory).name)):\n",
    "        inner_directory = os.path.join(directory, Path(directory).name)\n",
    "        xmls = glob(os.path.join(inner_directory, '*.csv'))\n",
    "        for xml in xmls:\n",
    "            shutil.move(xml, \n",
    "                        os.path.join(directory, Path(xml).name.replace(\"china_cities_\", \"\")))\n",
    "        os.remove(inner_directory)\n",
    "    else:\n",
    "        xmls = glob(os.path.join(directory, '*.csv'))\n",
    "        for xml in xmls:\n",
    "            os.rename(xml, xml.replace(\"china_cities_\", \"\"))\n",
    "    os.rename(directory, os.path.join(Path(directory).parent, Path(directory).name[3:7]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aca872e",
   "metadata": {},
   "source": [
    "#### 1.2 Get a directory tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33608949",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich.tree import Tree\n",
    "from rich import print\n",
    "import os\n",
    "\n",
    "def build_tree(path, tree):\n",
    "    for item in sorted(os.listdir(path)):\n",
    "        full_path = os.path.join(path, item)\n",
    "        if os.path.isdir(full_path):\n",
    "            branch = tree.add(f\"[bold blue]{item}/\")\n",
    "            build_tree(full_path, branch)\n",
    "        else:\n",
    "            tree.add(item)\n",
    "\n",
    "root_path = r\".\\Data\"\n",
    "tree = Tree(f\"[bold green]{os.path.basename(root_path)}/\")\n",
    "build_tree(root_path, tree)\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4220dd39",
   "metadata": {},
   "source": [
    "#### 1.3 Process all csv (columns change to rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76514a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3958/3958 [07:37<00:00,  8.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 转换成功，保存为：Data/2016/20160810_long.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 设置你的输入文件路径和输出路径\n",
    "input_dir_root = \"./Data\"\n",
    "csvs = glob(os.path.join(input_dir_root, '*/*.csv'))\n",
    "# output_csv_path = \"./20140514_long.csv\"\n",
    "\n",
    "for csv in tqdm(csvs):\n",
    "    output_csv_path = os.path.join(Path(csv).parent, f\"{Path(csv).stem}_long.csv\")\n",
    "\n",
    "    # 读取原始宽格式 CSV 数据\n",
    "    df = pd.read_csv(csv)\n",
    "\n",
    "    # 将从第4列（索引为3）开始的所有列名作为城市列\n",
    "    city_columns = df.columns[3:]\n",
    "\n",
    "    # 使用 pd.melt 进行宽转长\n",
    "    long_df = pd.melt(\n",
    "        df,\n",
    "        id_vars=[\"date\", \"hour\", \"type\"],\n",
    "        value_vars=city_columns,\n",
    "        var_name=\"city\",\n",
    "        value_name=\"value\"\n",
    "    )\n",
    "\n",
    "    # === 去除 value 为 NaN 的行 ===\n",
    "    long_df = long_df.dropna(subset=[\"value\"])\n",
    "\n",
    "    # 保存为新的长格式 CSV 文件\n",
    "    long_df.to_csv(output_csv_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"✅ 转换成功，保存为：{output_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed81f64a",
   "metadata": {},
   "source": [
    "#### 1.4 Add auxiliary csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b9d38e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3958/3958 [02:40<00:00, 24.67it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "# 设置你的输入文件路径和输出路径\n",
    "input_dir_root = \"./Data\"\n",
    "csvs = glob(os.path.join(input_dir_root, '*/*long.csv'))\n",
    "\n",
    "valid_dates = dict()\n",
    "valid_cities = dict()\n",
    "\n",
    "for csv in tqdm(csvs):\n",
    "    date = Path(csv).stem.strip('_long')\n",
    "    valid_dates[date] = list()\n",
    "    \n",
    "    df = pd.read_csv(csv)\n",
    "    cities = set(df['city'])\n",
    "    for city in cities:\n",
    "        valid_cities[city] = date\n",
    "    \n",
    "    valid_dates[date].append(list(cities))\n",
    "\n",
    "with open('./dates_to_cities_index.json', 'w+') as f:\n",
    "    json.dump(valid_dates, f, ensure_ascii=False, indent=4)\n",
    "with open('./cities_to_dates_index.json', 'w+') as f:\n",
    "    json.dump(valid_cities, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17e188c",
   "metadata": {},
   "source": [
    "#### 2.1 Due to large amount of data, we average the air quality metrics from each hour in one day to only one value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "468afc7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3958/3958 [07:56<00:00,  8.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理完成\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "data_root = r\".\\Data\"\n",
    "csvs = glob(os.path.join(data_root, '*/*.csv'))\n",
    "for csv in tqdm(csvs):\n",
    "\n",
    "    # 读取原始 CSV 文件\n",
    "    df = pd.read_csv(csv)\n",
    "\n",
    "    # 只保留我们感兴趣的 type\n",
    "    target_types = ['AQI', 'PM2.5', 'PM10', 'SO2', 'NO2', 'O3', 'CO']\n",
    "    df = df[df['type'].isin(target_types)]\n",
    "\n",
    "    # 计算每个城市、每种污染物的当天平均值\n",
    "    avg_df = df.groupby(['city', 'type'])['value'].mean().reset_index()\n",
    "    avg_df['value'] = avg_df['value'].round(2)\n",
    "\n",
    "    # 保存为新的 CSV 文件\n",
    "    avg_df.to_csv(csv, index=False)\n",
    "\n",
    "print(\"处理完成\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea5b5af",
   "metadata": {},
   "source": [
    "#### 2.2 Average again on months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bddd2835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "保存 Data\\2014\\2014_05.csv 完成，共处理 19 个文件\n",
      "保存 Data\\2014\\2014_06.csv 完成，共处理 30 个文件\n",
      "保存 Data\\2014\\2014_07.csv 完成，共处理 22 个文件\n",
      "保存 Data\\2014\\2014_08.csv 完成，共处理 31 个文件\n",
      "保存 Data\\2014\\2014_09.csv 完成，共处理 30 个文件\n",
      "保存 Data\\2014\\2014_10.csv 完成，共处理 31 个文件\n",
      "保存 Data\\2014\\2014_11.csv 完成，共处理 30 个文件\n",
      "保存 Data\\2014\\2014_12.csv 完成，共处理 31 个文件\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "def process_monthly_avg(data_root='Data', year='2015'):\n",
    "    year_path = os.path.join(data_root, year)\n",
    "    months = {f'{m:02d}': [] for m in range(1, 13)}\n",
    "\n",
    "    # 找出所有 *_summary.csv 文件\n",
    "    all_files = glob.glob(os.path.join(year_path, '*.csv'))\n",
    "\n",
    "    # 将文件按月份分类\n",
    "    for file in all_files:\n",
    "        basename = os.path.basename(file)\n",
    "        if len(basename) == 12:  # e.g., 20150101_summary.csv\n",
    "            month = basename[4:6]\n",
    "            if month in months:\n",
    "                months[month].append(file)\n",
    "\n",
    "    # 遍历每个月的文件进行聚合\n",
    "    for month, file_list in months.items():\n",
    "        if not file_list:\n",
    "            continue  # 当前月份无文件，跳过\n",
    "\n",
    "        monthly_df = pd.concat([pd.read_csv(f) for f in file_list])\n",
    "        monthly_avg = monthly_df.groupby(['city', 'type'])['value'].mean().reset_index()\n",
    "        monthly_avg['value'] = monthly_avg['value'].round(2)  # 保留两位小数\n",
    "\n",
    "        # 输出文件保存\n",
    "        output_file = os.path.join(year_path, f'{year}_{month}.csv')\n",
    "        monthly_avg.to_csv(output_file, index=False)\n",
    "        print(f'保存 {output_file} 完成，共处理 {len(file_list)} 个文件')\n",
    "\n",
    "# 运行示例：处理2015年数据\n",
    "years = ['2014', ]\n",
    "# '2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022', '2023', '2024', '2025']\n",
    "for year in years:\n",
    "    process_monthly_avg(data_root='Data', year=year)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717dd177",
   "metadata": {},
   "source": [
    "#### 2.3 Merge months data into one file for each year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80676543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "保存合并结果至：Data\\2015\\2015_all.csv\n",
      "保存合并结果至：Data\\2016\\2016_all.csv\n",
      "保存合并结果至：Data\\2017\\2017_all.csv\n",
      "保存合并结果至：Data\\2018\\2018_all.csv\n",
      "保存合并结果至：Data\\2019\\2019_all.csv\n",
      "保存合并结果至：Data\\2020\\2020_all.csv\n",
      "保存合并结果至：Data\\2021\\2021_all.csv\n",
      "保存合并结果至：Data\\2022\\2022_all.csv\n",
      "保存合并结果至：Data\\2023\\2023_all.csv\n",
      "保存合并结果至：Data\\2024\\2024_all.csv\n",
      "保存合并结果至：Data\\2025\\2025_all.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "def merge_yearly_monthly_averages(data_root='Data', year='2014'):\n",
    "    year_path = os.path.join(data_root, year)\n",
    "    pattern = os.path.join(year_path, f'{year}_*.csv')\n",
    "    files = sorted(glob.glob(pattern))\n",
    "\n",
    "    all_data = []\n",
    "\n",
    "    for file in files:\n",
    "        df = pd.read_csv(file)\n",
    "        # 提取月份（例如从 2014_05_monthly_avg.csv 提取 \"201405\"）\n",
    "        filename = os.path.basename(file)\n",
    "        month_str = filename.split('_')[1]\n",
    "        df.insert(1, 'month', f'{year}{month_str}')\n",
    "        all_data.append(df)\n",
    "\n",
    "    if all_data:\n",
    "        combined_df = pd.concat(all_data, ignore_index=True)\n",
    "        output_file = os.path.join(year_path, f'{year}_all.csv')\n",
    "        combined_df.to_csv(output_file, index=False)\n",
    "        print(f'保存合并结果至：{output_file}')\n",
    "    else:\n",
    "        print(f'未在 {year_path} 找到任何月平均文件')\n",
    "\n",
    "# 示例调用\n",
    "years = ['2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022', '2023', '2024', '2025']\n",
    "for year in years:\n",
    "    merge_yearly_monthly_averages(data_root='Data', year=year)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26736a9f",
   "metadata": {},
   "source": [
    "#### 2.3 Merge all annual files into one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "648b9898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "整合完成，保存为：./Data/all_data.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "def merge_all_years(data_root='Data', output_file='all_data.csv'):\n",
    "    pattern = os.path.join(data_root, '*/*_all.csv')\n",
    "    files = sorted(glob.glob(pattern))\n",
    "\n",
    "    all_data = []\n",
    "\n",
    "    for file in files:\n",
    "        df = pd.read_csv(file)\n",
    "        # 重命名 'month' 列为 'date'\n",
    "        df.rename(columns={'month': 'date'}, inplace=True)\n",
    "        all_data.append(df)\n",
    "\n",
    "    if all_data:\n",
    "        combined_df = pd.concat(all_data, ignore_index=True)\n",
    "        combined_df.to_csv(output_file, index=False)\n",
    "        print(f'整合完成，保存为：{output_file}')\n",
    "    else:\n",
    "        print(f'未在 {data_root} 中找到 *_all_months.csv 文件')\n",
    "\n",
    "# 示例调用\n",
    "merge_all_years(data_root='Data', output_file='./Data/all_data.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2ef4da",
   "metadata": {},
   "source": [
    "#### 3. Translate all Chinese into English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae79d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "from googletrans import Translator\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "nest_asyncio.apply()\n",
    "\n",
    "translator = Translator()\n",
    "csv = r\"./Data/all_data.csv\"\n",
    "df = pd.read_csv(csv)\n",
    "column = 'city'\n",
    "\n",
    "# 异步翻译函数\n",
    "async def translate_text(text, src='zh-cn', dest='en'):\n",
    "    try:\n",
    "        result = await translator.translate(text, src=src, dest=dest)\n",
    "        return result.text\n",
    "    except Exception as e:\n",
    "        print(f\"翻译失败: {text}，错误：{e}\")\n",
    "        return None\n",
    "\n",
    "# 主异步函数，处理整个 DataFrame 的翻译\n",
    "async def translate_column(df, column_name, new_column_name):\n",
    "    tasks = [translate_text(row[column_name]) for _, row in df.iterrows()]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    df[new_column_name] = results\n",
    "    return df\n",
    "\n",
    "loop = asyncio.get_event_loop()\n",
    "df_translated = asyncio.run(translate_column(df, column, 'city_en'))\n",
    "df_translated.to_csv(r\"./Data/all_data_trans.csv\", index=False, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877d20d4",
   "metadata": {},
   "source": [
    "#### 3.1 For Bubble chart, we need to create a new json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80493564",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# === 1. 读取合并后的 CSV 文件 ===\n",
    "file_path = \"./Data/merged_all_data_en.csv\"  # 替换为你的实际路径\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# === 2. 转换日期格式，并提取 \"年月\" ===\n",
    "dates = df['date'].copy().apply(lambda x: str(x)) \n",
    "df['date'] = pd.to_datetime(dates, format=\"%Y%m\", errors='coerce')\n",
    "df['year_month'] = df['date'].dt.to_period('M').astype(str)\n",
    "\n",
    "# === 3. 获取所有污染物类型 ===\n",
    "pollutants = df['type'].unique()\n",
    "\n",
    "# === 4. 针对每种污染物，计算每个省 + 月份的平均值 ===\n",
    "agg_frames = []\n",
    "for pollutant in pollutants:\n",
    "    sub_df = df[df['type'] == pollutant]\n",
    "    grouped = sub_df.groupby(['province', 'year_month'])['value'].mean().reset_index()\n",
    "    grouped.rename(columns={'value': pollutant}, inplace=True)\n",
    "    agg_frames.append(grouped)\n",
    "\n",
    "# === 5. 依次合并多个污染物的聚合结果 ===\n",
    "from functools import reduce\n",
    "merged_pollution = reduce(lambda left, right: pd.merge(left, right, on=['province', 'year_month'], how='outer'), agg_frames)\n",
    "\n",
    "# === 6. 添加静态字段：人口、GDP、地区等 ===\n",
    "static_fields = df[['province', 'population', 'gdp', 'region', 'climate_type', 'is_coastal']].drop_duplicates('province')\n",
    "final = pd.merge(merged_pollution, static_fields, on='province', how='left')\n",
    "\n",
    "# === 7. 可选导出为 JSON 或 CSV 文件 ===\n",
    "final.to_csv(\"./Data/bubble_data_full_en.csv\", index=False)\n",
    "# final.to_json(\"bubble_data_full.json\", orient=\"records\", force_ascii=False)\n",
    "\n",
    "# === 8. 输出预览前几行确认结果 ===\n",
    "# print(final.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97f82d7",
   "metadata": {},
   "source": [
    "#### Final Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68f335b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1/32 [00:00<00:23,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "翻译 河南省 为 Henan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 2/32 [00:01<00:22,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "翻译 山西省 为 Shanxi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 3/32 [00:02<00:21,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "翻译 山东省 为 Shandong\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 4/32 [00:03<00:21,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "翻译 广东省 为 Guangdong\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 5/32 [00:04<00:29,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "翻译 内蒙古自治区 为 Inner\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 6/32 [00:06<00:34,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "翻译 四川省 为 Sichuan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 7/32 [00:08<00:28,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "翻译 云南省 为 Yunnan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'expandtabs'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m translated_cities \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m city \u001b[38;5;129;01min\u001b[39;00m tqdm(cities):\n\u001b[1;32m---> 16\u001b[0m     transtext \u001b[38;5;241m=\u001b[39m \u001b[43mtranslator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranslate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcity\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m     transtext \u001b[38;5;241m=\u001b[39m transtext\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m翻译 \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcity\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m 为 \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtranstext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Anaconda\\Lib\\site-packages\\translate\\translate.py:44\u001b[0m, in \u001b[0;36mTranslator.translate\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_lang \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_lang:\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m text\n\u001b[1;32m---> 44\u001b[0m text_list \u001b[38;5;241m=\u001b[39m \u001b[43mwrap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTRANSLATION_API_MAX_LENGHT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplace_whitespace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprovider\u001b[38;5;241m.\u001b[39mget_translation(text_wraped) \u001b[38;5;28;01mfor\u001b[39;00m text_wraped \u001b[38;5;129;01min\u001b[39;00m text_list)\n",
      "File \u001b[1;32mc:\\Anaconda\\Lib\\textwrap.py:384\u001b[0m, in \u001b[0;36mwrap\u001b[1;34m(text, width, **kwargs)\u001b[0m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Wrap a single paragraph of text, returning a list of wrapped lines.\u001b[39;00m\n\u001b[0;32m    375\u001b[0m \n\u001b[0;32m    376\u001b[0m \u001b[38;5;124;03mReformat the single paragraph in 'text' so it fits in lines of no\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;124;03mwrapping behaviour.\u001b[39;00m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    383\u001b[0m w \u001b[38;5;241m=\u001b[39m TextWrapper(width\u001b[38;5;241m=\u001b[39mwidth, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 384\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Anaconda\\Lib\\textwrap.py:356\u001b[0m, in \u001b[0;36mTextWrapper.wrap\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrap\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[0;32m    348\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"wrap(text : string) -> [string]\u001b[39;00m\n\u001b[0;32m    349\u001b[0m \n\u001b[0;32m    350\u001b[0m \u001b[38;5;124;03m    Reformat the single paragraph in 'text' so it fits in lines of\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;124;03m    converted to space.\u001b[39;00m\n\u001b[0;32m    355\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 356\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_split_chunks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    357\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfix_sentence_endings:\n\u001b[0;32m    358\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fix_sentence_endings(chunks)\n",
      "File \u001b[1;32mc:\\Anaconda\\Lib\\textwrap.py:342\u001b[0m, in \u001b[0;36mTextWrapper._split_chunks\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_split_chunks\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[1;32m--> 342\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_munge_whitespace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_split(text)\n",
      "File \u001b[1;32mc:\\Anaconda\\Lib\\textwrap.py:151\u001b[0m, in \u001b[0;36mTextWrapper._munge_whitespace\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"_munge_whitespace(text : string) -> string\u001b[39;00m\n\u001b[0;32m    145\u001b[0m \n\u001b[0;32m    146\u001b[0m \u001b[38;5;124;03mMunge whitespace in text: expand tabs and convert all other\u001b[39;00m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;124;03mwhitespace characters to spaces.  Eg. \" foo\\\\tbar\\\\n\\\\nbaz\"\u001b[39;00m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;124;03mbecomes \" foo    bar  baz\".\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpand_tabs:\n\u001b[1;32m--> 151\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[43mtext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpandtabs\u001b[49m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtabsize)\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplace_whitespace:\n\u001b[0;32m    153\u001b[0m     text \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mtranslate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39municode_whitespace_trans)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'float' object has no attribute 'expandtabs'"
     ]
    }
   ],
   "source": [
    "from translate import Translator\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "\n",
    "csv_path = r\"./Data/merged_all_data_en.csv\"\n",
    "out = r\"./Data/merged_all_data_en2.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "column = 'province'\n",
    "translator = Translator(from_lang='zh', to_lang='en')\n",
    "cities = list(set(df[column]))\n",
    "translated_cities = dict()\n",
    "for city in tqdm(cities):\n",
    "    transtext = translator.translate(city)\n",
    "    transtext = transtext.split(' ')[0]\n",
    "    print(f\"翻译 {city} 为 {transtext}\")\n",
    "    translated_cities[city] = transtext\n",
    "df[column] = df[column].map(translated_cities)\n",
    "df.to_csv(out, index=False, encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
