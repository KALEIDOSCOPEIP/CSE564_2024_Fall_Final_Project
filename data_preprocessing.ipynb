{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c6d0426",
   "metadata": {},
   "source": [
    "#### 1.1 Reorganize all csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3505970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.\\\\Data\\\\城市_20250101-20250329']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 142.03it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from glob import glob  \n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "data_root = r\".\\Data\"\n",
    "directories = glob(os.path.join(data_root, r'城市*'))\n",
    "print(directories)\n",
    "for directory in tqdm(directories):\n",
    "    if os.path.exists(os.path.join(directory, Path(directory).name)):\n",
    "        inner_directory = os.path.join(directory, Path(directory).name)\n",
    "        xmls = glob(os.path.join(inner_directory, '*.csv'))\n",
    "        for xml in xmls:\n",
    "            shutil.move(xml, \n",
    "                        os.path.join(directory, Path(xml).name.replace(\"china_cities_\", \"\")))\n",
    "        os.remove(inner_directory)\n",
    "    else:\n",
    "        xmls = glob(os.path.join(directory, '*.csv'))\n",
    "        for xml in xmls:\n",
    "            os.rename(xml, xml.replace(\"china_cities_\", \"\"))\n",
    "    os.rename(directory, os.path.join(Path(directory).parent, Path(directory).name[3:7]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aca872e",
   "metadata": {},
   "source": [
    "#### 1.2 Get a directory tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33608949",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich.tree import Tree\n",
    "from rich import print\n",
    "import os\n",
    "\n",
    "def build_tree(path, tree):\n",
    "    for item in sorted(os.listdir(path)):\n",
    "        full_path = os.path.join(path, item)\n",
    "        if os.path.isdir(full_path):\n",
    "            branch = tree.add(f\"[bold blue]{item}/\")\n",
    "            build_tree(full_path, branch)\n",
    "        else:\n",
    "            tree.add(item)\n",
    "\n",
    "root_path = r\".\\Data\"\n",
    "tree = Tree(f\"[bold green]{os.path.basename(root_path)}/\")\n",
    "build_tree(root_path, tree)\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4220dd39",
   "metadata": {},
   "source": [
    "#### 1.3 Process all csv (columns change to rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76514a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3958/3958 [07:37<00:00,  8.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 转换成功，保存为：Data/2016/20160810_long.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 设置你的输入文件路径和输出路径\n",
    "input_dir_root = \"./Data\"\n",
    "csvs = glob(os.path.join(input_dir_root, '*/*.csv'))\n",
    "# output_csv_path = \"./20140514_long.csv\"\n",
    "\n",
    "for csv in tqdm(csvs):\n",
    "    output_csv_path = os.path.join(Path(csv).parent, f\"{Path(csv).stem}_long.csv\")\n",
    "\n",
    "    # 读取原始宽格式 CSV 数据\n",
    "    df = pd.read_csv(csv)\n",
    "\n",
    "    # 将从第4列（索引为3）开始的所有列名作为城市列\n",
    "    city_columns = df.columns[3:]\n",
    "\n",
    "    # 使用 pd.melt 进行宽转长\n",
    "    long_df = pd.melt(\n",
    "        df,\n",
    "        id_vars=[\"date\", \"hour\", \"type\"],\n",
    "        value_vars=city_columns,\n",
    "        var_name=\"city\",\n",
    "        value_name=\"value\"\n",
    "    )\n",
    "\n",
    "    # === 去除 value 为 NaN 的行 ===\n",
    "    long_df = long_df.dropna(subset=[\"value\"])\n",
    "\n",
    "    # 保存为新的长格式 CSV 文件\n",
    "    long_df.to_csv(output_csv_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"✅ 转换成功，保存为：{output_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed81f64a",
   "metadata": {},
   "source": [
    "#### 1.4 Add auxiliary csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b9d38e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3958/3958 [02:40<00:00, 24.67it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "# 设置你的输入文件路径和输出路径\n",
    "input_dir_root = \"./Data\"\n",
    "csvs = glob(os.path.join(input_dir_root, '*/*long.csv'))\n",
    "\n",
    "valid_dates = dict()\n",
    "valid_cities = dict()\n",
    "\n",
    "for csv in tqdm(csvs):\n",
    "    date = Path(csv).stem.strip('_long')\n",
    "    valid_dates[date] = list()\n",
    "    \n",
    "    df = pd.read_csv(csv)\n",
    "    cities = set(df['city'])\n",
    "    for city in cities:\n",
    "        valid_cities[city] = date\n",
    "    \n",
    "    valid_dates[date].append(list(cities))\n",
    "\n",
    "with open('./dates_to_cities_index.json', 'w+') as f:\n",
    "    json.dump(valid_dates, f, ensure_ascii=False, indent=4)\n",
    "with open('./cities_to_dates_index.json', 'w+') as f:\n",
    "    json.dump(valid_cities, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17e188c",
   "metadata": {},
   "source": [
    "#### 2.1 Due to large amount of data, we average the air quality metrics from each hour in one day to only one value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "468afc7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3958/3958 [07:56<00:00,  8.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理完成\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "data_root = r\".\\Data\"\n",
    "csvs = glob(os.path.join(data_root, '*/*.csv'))\n",
    "for csv in tqdm(csvs):\n",
    "\n",
    "    # 读取原始 CSV 文件\n",
    "    df = pd.read_csv(csv)\n",
    "\n",
    "    # 只保留我们感兴趣的 type\n",
    "    target_types = ['AQI', 'PM2.5', 'PM10', 'SO2', 'NO2', 'O3', 'CO']\n",
    "    df = df[df['type'].isin(target_types)]\n",
    "\n",
    "    # 计算每个城市、每种污染物的当天平均值\n",
    "    avg_df = df.groupby(['city', 'type'])['value'].mean().reset_index()\n",
    "    avg_df['value'] = avg_df['value'].round(2)\n",
    "\n",
    "    # 保存为新的 CSV 文件\n",
    "    avg_df.to_csv(csv, index=False)\n",
    "\n",
    "print(\"处理完成\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea5b5af",
   "metadata": {},
   "source": [
    "#### 2.2 Average again on months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bddd2835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "保存 Data\\2014\\2014_05.csv 完成，共处理 19 个文件\n",
      "保存 Data\\2014\\2014_06.csv 完成，共处理 30 个文件\n",
      "保存 Data\\2014\\2014_07.csv 完成，共处理 22 个文件\n",
      "保存 Data\\2014\\2014_08.csv 完成，共处理 31 个文件\n",
      "保存 Data\\2014\\2014_09.csv 完成，共处理 30 个文件\n",
      "保存 Data\\2014\\2014_10.csv 完成，共处理 31 个文件\n",
      "保存 Data\\2014\\2014_11.csv 完成，共处理 30 个文件\n",
      "保存 Data\\2014\\2014_12.csv 完成，共处理 31 个文件\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "def process_monthly_avg(data_root='Data', year='2015'):\n",
    "    year_path = os.path.join(data_root, year)\n",
    "    months = {f'{m:02d}': [] for m in range(1, 13)}\n",
    "\n",
    "    # 找出所有 *_summary.csv 文件\n",
    "    all_files = glob.glob(os.path.join(year_path, '*.csv'))\n",
    "\n",
    "    # 将文件按月份分类\n",
    "    for file in all_files:\n",
    "        basename = os.path.basename(file)\n",
    "        if len(basename) == 12:  # e.g., 20150101_summary.csv\n",
    "            month = basename[4:6]\n",
    "            if month in months:\n",
    "                months[month].append(file)\n",
    "\n",
    "    # 遍历每个月的文件进行聚合\n",
    "    for month, file_list in months.items():\n",
    "        if not file_list:\n",
    "            continue  # 当前月份无文件，跳过\n",
    "\n",
    "        monthly_df = pd.concat([pd.read_csv(f) for f in file_list])\n",
    "        monthly_avg = monthly_df.groupby(['city', 'type'])['value'].mean().reset_index()\n",
    "        monthly_avg['value'] = monthly_avg['value'].round(2)  # 保留两位小数\n",
    "\n",
    "        # 输出文件保存\n",
    "        output_file = os.path.join(year_path, f'{year}_{month}.csv')\n",
    "        monthly_avg.to_csv(output_file, index=False)\n",
    "        print(f'保存 {output_file} 完成，共处理 {len(file_list)} 个文件')\n",
    "\n",
    "# 运行示例：处理2015年数据\n",
    "years = ['2014', ]\n",
    "# '2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022', '2023', '2024', '2025']\n",
    "for year in years:\n",
    "    process_monthly_avg(data_root='Data', year=year)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
