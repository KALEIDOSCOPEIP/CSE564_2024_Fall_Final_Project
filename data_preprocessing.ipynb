{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c6d0426",
   "metadata": {},
   "source": [
    "#### 1.1 Reorganize all csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3505970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.\\\\Data\\\\城市_20250101-20250329']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 142.03it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from glob import glob  \n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "data_root = r\".\\Data\"\n",
    "directories = glob(os.path.join(data_root, r'城市*'))\n",
    "print(directories)\n",
    "for directory in tqdm(directories):\n",
    "    if os.path.exists(os.path.join(directory, Path(directory).name)):\n",
    "        inner_directory = os.path.join(directory, Path(directory).name)\n",
    "        xmls = glob(os.path.join(inner_directory, '*.csv'))\n",
    "        for xml in xmls:\n",
    "            shutil.move(xml, \n",
    "                        os.path.join(directory, Path(xml).name.replace(\"china_cities_\", \"\")))\n",
    "        os.remove(inner_directory)\n",
    "    else:\n",
    "        xmls = glob(os.path.join(directory, '*.csv'))\n",
    "        for xml in xmls:\n",
    "            os.rename(xml, xml.replace(\"china_cities_\", \"\"))\n",
    "    os.rename(directory, os.path.join(Path(directory).parent, Path(directory).name[3:7]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aca872e",
   "metadata": {},
   "source": [
    "#### 1.2 Get a directory tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33608949",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich.tree import Tree\n",
    "from rich import print\n",
    "import os\n",
    "\n",
    "def build_tree(path, tree):\n",
    "    for item in sorted(os.listdir(path)):\n",
    "        full_path = os.path.join(path, item)\n",
    "        if os.path.isdir(full_path):\n",
    "            branch = tree.add(f\"[bold blue]{item}/\")\n",
    "            build_tree(full_path, branch)\n",
    "        else:\n",
    "            tree.add(item)\n",
    "\n",
    "root_path = r\".\\Data\"\n",
    "tree = Tree(f\"[bold green]{os.path.basename(root_path)}/\")\n",
    "build_tree(root_path, tree)\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4220dd39",
   "metadata": {},
   "source": [
    "#### 1.3 Process all csv (columns change to rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76514a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3958/3958 [07:37<00:00,  8.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 转换成功，保存为：Data/2016/20160810_long.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 设置你的输入文件路径和输出路径\n",
    "input_dir_root = \"./Data\"\n",
    "csvs = glob(os.path.join(input_dir_root, '*/*.csv'))\n",
    "# output_csv_path = \"./20140514_long.csv\"\n",
    "\n",
    "for csv in tqdm(csvs):\n",
    "    output_csv_path = os.path.join(Path(csv).parent, f\"{Path(csv).stem}_long.csv\")\n",
    "\n",
    "    # 读取原始宽格式 CSV 数据\n",
    "    df = pd.read_csv(csv)\n",
    "\n",
    "    # 将从第4列（索引为3）开始的所有列名作为城市列\n",
    "    city_columns = df.columns[3:]\n",
    "\n",
    "    # 使用 pd.melt 进行宽转长\n",
    "    long_df = pd.melt(\n",
    "        df,\n",
    "        id_vars=[\"date\", \"hour\", \"type\"],\n",
    "        value_vars=city_columns,\n",
    "        var_name=\"city\",\n",
    "        value_name=\"value\"\n",
    "    )\n",
    "\n",
    "    # === 去除 value 为 NaN 的行 ===\n",
    "    long_df = long_df.dropna(subset=[\"value\"])\n",
    "\n",
    "    # 保存为新的长格式 CSV 文件\n",
    "    long_df.to_csv(output_csv_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"✅ 转换成功，保存为：{output_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed81f64a",
   "metadata": {},
   "source": [
    "#### 1.4 Add auxiliary csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b9d38e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3958/3958 [02:40<00:00, 24.67it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "# 设置你的输入文件路径和输出路径\n",
    "input_dir_root = \"./Data\"\n",
    "csvs = glob(os.path.join(input_dir_root, '*/*long.csv'))\n",
    "\n",
    "valid_dates = dict()\n",
    "valid_cities = dict()\n",
    "\n",
    "for csv in tqdm(csvs):\n",
    "    date = Path(csv).stem.strip('_long')\n",
    "    valid_dates[date] = list()\n",
    "    \n",
    "    df = pd.read_csv(csv)\n",
    "    cities = set(df['city'])\n",
    "    for city in cities:\n",
    "        valid_cities[city] = date\n",
    "    \n",
    "    valid_dates[date].append(list(cities))\n",
    "\n",
    "with open('./dates_to_cities_index.json', 'w+') as f:\n",
    "    json.dump(valid_dates, f, ensure_ascii=False, indent=4)\n",
    "with open('./cities_to_dates_index.json', 'w+') as f:\n",
    "    json.dump(valid_cities, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17e188c",
   "metadata": {},
   "source": [
    "#### 2.1 Due to large amount of data, we average the air quality metrics from each hour in one day to only one value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "468afc7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3958/3958 [07:56<00:00,  8.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理完成\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "data_root = r\".\\Data\"\n",
    "csvs = glob(os.path.join(data_root, '*/*.csv'))\n",
    "for csv in tqdm(csvs):\n",
    "\n",
    "    # 读取原始 CSV 文件\n",
    "    df = pd.read_csv(csv)\n",
    "\n",
    "    # 只保留我们感兴趣的 type\n",
    "    target_types = ['AQI', 'PM2.5', 'PM10', 'SO2', 'NO2', 'O3', 'CO']\n",
    "    df = df[df['type'].isin(target_types)]\n",
    "\n",
    "    # 计算每个城市、每种污染物的当天平均值\n",
    "    avg_df = df.groupby(['city', 'type'])['value'].mean().reset_index()\n",
    "    avg_df['value'] = avg_df['value'].round(2)\n",
    "\n",
    "    # 保存为新的 CSV 文件\n",
    "    avg_df.to_csv(csv, index=False)\n",
    "\n",
    "print(\"处理完成\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea5b5af",
   "metadata": {},
   "source": [
    "#### 2.2 Average again on months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bddd2835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "保存 Data\\2014\\2014_05.csv 完成，共处理 19 个文件\n",
      "保存 Data\\2014\\2014_06.csv 完成，共处理 30 个文件\n",
      "保存 Data\\2014\\2014_07.csv 完成，共处理 22 个文件\n",
      "保存 Data\\2014\\2014_08.csv 完成，共处理 31 个文件\n",
      "保存 Data\\2014\\2014_09.csv 完成，共处理 30 个文件\n",
      "保存 Data\\2014\\2014_10.csv 完成，共处理 31 个文件\n",
      "保存 Data\\2014\\2014_11.csv 完成，共处理 30 个文件\n",
      "保存 Data\\2014\\2014_12.csv 完成，共处理 31 个文件\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "def process_monthly_avg(data_root='Data', year='2015'):\n",
    "    year_path = os.path.join(data_root, year)\n",
    "    months = {f'{m:02d}': [] for m in range(1, 13)}\n",
    "\n",
    "    # 找出所有 *_summary.csv 文件\n",
    "    all_files = glob.glob(os.path.join(year_path, '*.csv'))\n",
    "\n",
    "    # 将文件按月份分类\n",
    "    for file in all_files:\n",
    "        basename = os.path.basename(file)\n",
    "        if len(basename) == 12:  # e.g., 20150101_summary.csv\n",
    "            month = basename[4:6]\n",
    "            if month in months:\n",
    "                months[month].append(file)\n",
    "\n",
    "    # 遍历每个月的文件进行聚合\n",
    "    for month, file_list in months.items():\n",
    "        if not file_list:\n",
    "            continue  # 当前月份无文件，跳过\n",
    "\n",
    "        monthly_df = pd.concat([pd.read_csv(f) for f in file_list])\n",
    "        monthly_avg = monthly_df.groupby(['city', 'type'])['value'].mean().reset_index()\n",
    "        monthly_avg['value'] = monthly_avg['value'].round(2)  # 保留两位小数\n",
    "\n",
    "        # 输出文件保存\n",
    "        output_file = os.path.join(year_path, f'{year}_{month}.csv')\n",
    "        monthly_avg.to_csv(output_file, index=False)\n",
    "        print(f'保存 {output_file} 完成，共处理 {len(file_list)} 个文件')\n",
    "\n",
    "# 运行示例：处理2015年数据\n",
    "years = ['2014', ]\n",
    "# '2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022', '2023', '2024', '2025']\n",
    "for year in years:\n",
    "    process_monthly_avg(data_root='Data', year=year)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717dd177",
   "metadata": {},
   "source": [
    "#### 2.3 Merge months data into one file for each year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80676543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "保存合并结果至：Data\\2015\\2015_all.csv\n",
      "保存合并结果至：Data\\2016\\2016_all.csv\n",
      "保存合并结果至：Data\\2017\\2017_all.csv\n",
      "保存合并结果至：Data\\2018\\2018_all.csv\n",
      "保存合并结果至：Data\\2019\\2019_all.csv\n",
      "保存合并结果至：Data\\2020\\2020_all.csv\n",
      "保存合并结果至：Data\\2021\\2021_all.csv\n",
      "保存合并结果至：Data\\2022\\2022_all.csv\n",
      "保存合并结果至：Data\\2023\\2023_all.csv\n",
      "保存合并结果至：Data\\2024\\2024_all.csv\n",
      "保存合并结果至：Data\\2025\\2025_all.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "def merge_yearly_monthly_averages(data_root='Data', year='2014'):\n",
    "    year_path = os.path.join(data_root, year)\n",
    "    pattern = os.path.join(year_path, f'{year}_*.csv')\n",
    "    files = sorted(glob.glob(pattern))\n",
    "\n",
    "    all_data = []\n",
    "\n",
    "    for file in files:\n",
    "        df = pd.read_csv(file)\n",
    "        # 提取月份（例如从 2014_05_monthly_avg.csv 提取 \"201405\"）\n",
    "        filename = os.path.basename(file)\n",
    "        month_str = filename.split('_')[1]\n",
    "        df.insert(1, 'month', f'{year}{month_str}')\n",
    "        all_data.append(df)\n",
    "\n",
    "    if all_data:\n",
    "        combined_df = pd.concat(all_data, ignore_index=True)\n",
    "        output_file = os.path.join(year_path, f'{year}_all.csv')\n",
    "        combined_df.to_csv(output_file, index=False)\n",
    "        print(f'保存合并结果至：{output_file}')\n",
    "    else:\n",
    "        print(f'未在 {year_path} 找到任何月平均文件')\n",
    "\n",
    "# 示例调用\n",
    "years = ['2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022', '2023', '2024', '2025']\n",
    "for year in years:\n",
    "    merge_yearly_monthly_averages(data_root='Data', year=year)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26736a9f",
   "metadata": {},
   "source": [
    "#### 2.3 Merge all annual files into one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "648b9898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "整合完成，保存为：./Data/all_data.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "def merge_all_years(data_root='Data', output_file='all_data.csv'):\n",
    "    pattern = os.path.join(data_root, '*/*_all.csv')\n",
    "    files = sorted(glob.glob(pattern))\n",
    "\n",
    "    all_data = []\n",
    "\n",
    "    for file in files:\n",
    "        df = pd.read_csv(file)\n",
    "        # 重命名 'month' 列为 'date'\n",
    "        df.rename(columns={'month': 'date'}, inplace=True)\n",
    "        all_data.append(df)\n",
    "\n",
    "    if all_data:\n",
    "        combined_df = pd.concat(all_data, ignore_index=True)\n",
    "        combined_df.to_csv(output_file, index=False)\n",
    "        print(f'整合完成，保存为：{output_file}')\n",
    "    else:\n",
    "        print(f'未在 {data_root} 中找到 *_all_months.csv 文件')\n",
    "\n",
    "# 示例调用\n",
    "merge_all_years(data_root='Data', output_file='./Data/all_data.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2ef4da",
   "metadata": {},
   "source": [
    "#### 3. Translate all Chinese into English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae79d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "from googletrans import Translator\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "nest_asyncio.apply()\n",
    "\n",
    "translator = Translator()\n",
    "csv = r\"./Data/all_data.csv\"\n",
    "df = pd.read_csv(csv)\n",
    "column = 'city'\n",
    "\n",
    "# 异步翻译函数\n",
    "async def translate_text(text, src='zh-cn', dest='en'):\n",
    "    try:\n",
    "        result = await translator.translate(text, src=src, dest=dest)\n",
    "        return result.text\n",
    "    except Exception as e:\n",
    "        print(f\"翻译失败: {text}，错误：{e}\")\n",
    "        return None\n",
    "\n",
    "# 主异步函数，处理整个 DataFrame 的翻译\n",
    "async def translate_column(df, column_name, new_column_name):\n",
    "    tasks = [translate_text(row[column_name]) for _, row in df.iterrows()]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    df[new_column_name] = results\n",
    "    return df\n",
    "\n",
    "loop = asyncio.get_event_loop()\n",
    "df_translated = asyncio.run(translate_column(df, column, 'city_en'))\n",
    "df_translated.to_csv(r\"./Data/all_data_trans.csv\", index=False, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877d20d4",
   "metadata": {},
   "source": [
    "#### 3.1 For Bubble chart, we need to create a new json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80493564",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# === 1. 读取合并后的 CSV 文件 ===\n",
    "file_path = \"./Data/merged_all_data_sample.csv\"  # 替换为你的实际路径\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# === 2. 转换日期格式，并提取 \"年月\" ===\n",
    "df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "df['year_month'] = df['date'].dt.to_period('M').astype(str)\n",
    "\n",
    "# === 3. 获取所有污染物类型 ===\n",
    "pollutants = df['type'].unique()\n",
    "\n",
    "# === 4. 针对每种污染物，计算每个省 + 月份的平均值 ===\n",
    "agg_frames = []\n",
    "for pollutant in pollutants:\n",
    "    sub_df = df[df['type'] == pollutant]\n",
    "    grouped = sub_df.groupby(['province', 'year_month'])['value'].mean().reset_index()\n",
    "    grouped.rename(columns={'value': pollutant}, inplace=True)\n",
    "    agg_frames.append(grouped)\n",
    "\n",
    "# === 5. 依次合并多个污染物的聚合结果 ===\n",
    "from functools import reduce\n",
    "merged_pollution = reduce(lambda left, right: pd.merge(left, right, on=['province', 'year_month'], how='outer'), agg_frames)\n",
    "\n",
    "# === 6. 添加静态字段：人口、GDP、地区等 ===\n",
    "static_fields = df[['province', 'population', 'gdp', 'region', 'climate_type', 'is_coastal']].drop_duplicates('province')\n",
    "final = pd.merge(merged_pollution, static_fields, on='province', how='left')\n",
    "\n",
    "# === 7. 可选导出为 JSON 或 CSV 文件 ===\n",
    "final.to_csv(\"./Data/bubble_data_full.csv\", index=False)\n",
    "# final.to_json(\"bubble_data_full.json\", orient=\"records\", force_ascii=False)\n",
    "\n",
    "# === 8. 输出预览前几行确认结果 ===\n",
    "# print(final.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
